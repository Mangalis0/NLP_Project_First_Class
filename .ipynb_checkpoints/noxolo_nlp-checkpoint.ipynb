{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules for data science and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# NLP Libraries\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the datasets\n",
    "train = pd.read_csv(\"train.csv\").fillna(' ')\n",
    "test = pd.read_csv(\"test.csv\").fillna(' ')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution\n",
    "train['sentiment'].value_counts()/train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of the target \n",
    "plt.hist(train['sentiment'], label='data');\n",
    "plt.legend();\n",
    "plt.title('Distribution of target labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is notable that there is an imbalance of classes in the daset  and these tend to be a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. This is can cause problems as most algorithms are designed to maximize accuracy and reduce error. Therefore we ill address and take care of the class imbalance in our EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "text = \" \".join(tweet for tweet in train.message)\n",
    "print (\"There are {} words in the combination of all tweets.\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "# Displaying the word cloud image:\n",
    "# using matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word clouds are a simple visualization of data, in which words are shown in varying sizes depending on how often they appear. The kind of insight they provide even without much analysis being done helps in data normalization and in also what to expect as we dive deeper into data analysis. In our case, it is no surprise that both \"climate change\" and \"global warming\" as the biggest words in the could. Some other big words include \"https\" and \"RT\" which can be easily categorized as potential stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with Database normalization which is the process of structuring a relational database in accordance with a series of normal forms in order to reduce data redundancy. This will aslo help improve data integrity as it entails organizing the attributes of a dataset to ensure that their dependencies are properly enforced by database integrity constraints.\n",
    "\n",
    "Stemming and Lemmatization are Text Normalization techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stemmer will return the stem of a word, which needn't be identical to the morphological root of the word. It usually sufficient that related words map to the same stem,even if the stem is not in itself a valid root, while in lemmatisation, it will return the dictionary form of a word, which must be a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the PorterStemmer \n",
    "stemmer = PorterStemmer()\n",
    "print(\"The stemmed form of typing is: {}\".format(stemmer.stem(\"typing\")))\n",
    "print(\"The stemmed form of types is: {}\".format(stemmer.stem(\"types\")))\n",
    "print(\"The stemmed form of type is: {}\".format(stemmer.stem(\"type\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lemmatisation, the part of speech of a word should be first determined and the normalisation rules will be different for different part of speech, while the stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"In  case of Lemmatization, typing is: {}\".format(lemm.lemmatize(\"typing\")))\n",
    "print(\"In  case of Lemmatization, types is: {}\".format(lemm.lemmatize(\"types\")))\n",
    "print(\"In  case of Lemmatization, type is: {}\".format(lemm.lemmatize(\"type\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important step in the data mining process. In our case of classification, preprocessing data means; Data cleaning, Fill in missing values, smoothing noisy data and resolving any existing inconsistencies. Success in the steps will then make it possible and some-what easy for us to perform Data integration, Data transformation, Normalization and aggregation as well as Data reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessor(df):\n",
    "    '''\n",
    "    For preprocessing we have regularized, transformed each upper case into lower case, tokenized,\n",
    "    Normalized and remove stopwords. For normalization, we have used PorterStemmer. \n",
    "    Porter stemmer transforms a sentence from this \"love loving loved\" to this \"love love love\"\n",
    "    \n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #stop_words.append(RT)\n",
    "    stemmer = PorterStemmer()\n",
    "    #lemm = WordNetLemmatizer()\n",
    "    Tokenized_Doc=[]\n",
    "    print(\"Preprocessing data.........\\n\")\n",
    "    for data in df['message']:\n",
    "        review = re.sub('[^a-zA-Z]', ' ', data)\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        review = url.sub(r'',review)\n",
    "        html=re.compile(r'<.*?>')\n",
    "        review = html.sub(r'',review)\n",
    "        emojis = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        review = emojis.sub(r'',review)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        #gen_tweets = [stemmer.stem(token) for token in tokens if not token in stop_words]\n",
    "        gen_tweets = [lemm.lemmatize(token) for token in tokens if not token in stop_words]\n",
    "        cleaned =' '.join(gen_tweets)\n",
    "        Tokenized_Doc.append(gen_tweets)\n",
    "        df['tweet tokens'] = pd.Series(Tokenized_Doc)\n",
    "        #df.insert(loc=2, column='tweet tokens', value=Tokenized_Doc)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_preprocessor(train)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['tweet tokens']\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in data] #create your corpus here\n",
    "\n",
    "vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATA AND INITIALZING THE LOGISTIC REGRESSION CLASSIFIER\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scikit_log_reg = LogisticRegression(solver='liblinear',random_state=42) #, C=5, penalty='l2',max_iter=1000)\n",
    "model=scikit_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PREDICTIONS\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVERSE CATEGORIES - DESCENDING ORDER OF IMPORTANCE\n",
    "preds=[ item[::-1] for item in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PREDICTIONS: PROBABILITIES\n",
    "probs = model.predict_proba(X_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TOP K PREDICTIONS BY PROB - note these are just index\n",
    "best_n = np.argsort(probs, axis=1)[:,-5:]\n",
    "best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET CATEGORY OF PREDICTIONS\n",
    "preds_prob=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "preds_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Confusion Matrix which is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then fllowed by a Classification report which is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. \n",
    "More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "test_df = data_preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "data2 = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in data2] #create your corpus here\n",
    "\n",
    "tests = vectorizer.transform(corpus, copy=True)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(tests.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same model to do the predictions\n",
    "pred = model.predict(tests)\n",
    "predictions = pred[:]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
